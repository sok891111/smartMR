import streamlit as st
import pandas as pd
from menu import menu_with_redirect
# import airtable
from pyairtable import Api
from openai import OpenAI
import json
import copy
# Redirect to app.py if not logged in, otherwise show the navigation menu
st.set_page_config(layout="wide", page_title='ìì¬ ë°œì£¼ ì‹œìŠ¤í…œ(smartMR)')
menu_with_redirect()



api_key = 'patqEfN06y9QLUoS0.0ff77915af3ff09a6aea08f2b64da60016c567129cea7448c8d903265d5cda71'

base_id= 'appESzks3l70ngxgd'
cosmax_table = 'tbloZpZ5QZY6nZPD0'
component_table = 'tbl8uwQUhuWQMlJtf'
product_table= 'tblreG6m3bTRYMeLy'

#1. ì œí’ˆ ì •ë³´
@st.cache_data(ttl=1200)  
def get_product_table_list():
    api = Api(api_key)
    table = api.table(base_id, product_table)
    data = pd.DataFrame(r['fields'] for r in table.all())
    return data


def get_product_list(product_name =None, product_code=None):
    data = get_product_table_list()
    query = ''
    
    if product_name != '' and  product_name is not None:
        query += f'ì œí’ˆì½”ë“œ.str.contains("{product_name}" , case=False, na=False)'
    if product_code != '' and product_code is not None :
        if query != '':
            query+= ' & '
        query += f'ì œí’ˆì½”ë“œ == "{product_code}"'
    
    return data.query(query)


#2. ìì¬ ì •ë³´
@st.cache_data(ttl=1200)  
def get_component_table_list():
    api = Api(api_key)
    table = api.table(base_id, component_table)
    data = pd.DataFrame(r['fields'] for r in table.all())
    return data


def get_component_list(component_name =None, component_code=None , component_maker=None):
    data = get_component_table_list()
    query = ''
    
    if component_name != '' and  component_name is not None:
        query += f'ìì¬ëª….str.contains("{component_name}" , case=False, na=False)'
    if component_code != '' and component_code is not None :
        if query != '':
            query+= ' & '
        query += f'ìì¬ì½”ë“œ == "{component_code}"'

    if component_maker != '' and component_maker is not None :
        if query != '':
            query+= ' & '
        query += f'ì—…ì²´ì½”ë“œ.str.contains("{component_maker}" , case=False, na=False)'
    print(query)
    return data.query(query)



### llm ì •ë³´
llm_key = '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'
client = OpenAI(api_key=llm_key , base_url = 'https://api.groq.com/openai/v1')

system_prompt ='''ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI assistant ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì •ë³´ë¥¼ í™œìš©í•˜ê±°ë‚˜, tool ì„ í™œìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ í•œêµ­ì–´ë¡œë§Œ í•´ì£¼ì„¸ìš”. ì‚¬ìš©ìê°€ í•œêµ­ì¸ì…ë‹ˆë‹¤.
'''


tools = [
    {
        "type": "function",
        "function": {
            "name": "get_product_list",
            "description": '''ì„¸ë¥´ìŸŒëŠ(cherejeanne) ì˜ ìƒí’ˆ,ì œí’ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.''',
            "parameters": {
                "type": "object",
                "properties": {
                    "product_name": {"type": "string", "description": "ì œí’ˆëª… ( ex. íë“œë•… ë£¨ë¯¸ì—ë¥´ ë„ë ˆ ì¿ ì…˜ ). ì œí’ˆì˜ ì´ë¦„"},
                    "product_code": {"type": "string", "description": "ì œí’ˆ ì½”ë“œ( ex. 9CPS0000610 ) ì½”ë“œ ì •ë³´"}
                },
                "required": ["product_name", "product_code"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "get_component_list",
            "description": '''ì„¸ë¥´ìŸŒëŠ(cherejeanne) ì˜ ìì¬(ë¶€í’ˆ) ë¦¬ìŠ¤íŠ¸ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.''',
            "parameters": {
                "type": "object",
                "properties": {
                    "component_name": {"type": "string", "description": "ìì¬ëª… ( ex. ì¿ í¼ìŠ¤í…Œì´ì…˜ì…°ë¥´ì”ëŠíë“œë•…ë£¨ë¯¸ì—ë¥´ë„ë ˆì¿ ì…˜ ). ìì¬ì˜ ì´ë¦„"},
                    "component_code": {"type": "string", "description": "ìì¬ ì½”ë“œ( ex. 7CPS000041N00 ) ìì¬ ì •ë³´"},
                    "component_maker": {"type": "string", "description": "ì—…ì²´ëª… ( ex. ë‘ë³´ì‚°ì—… ) ìì¬ë¥¼ ë§Œë“œëŠ” ì—…ì²´ì´ë¦„, ì—…ì²´ ì •ë³´, vendoer, maker ì •ë³´"},
                },
                "required": ["component_name", "component_code" , "component_maker"]
            }
        }
    }



]


def add_numbers(a, b):
    return a + b
  

def get_openai_stream(prompt):
    response_chunks = []
    messages = st.session_state.messages[:]
    messages.insert(0 , {"role": "system", "content": system_prompt})
    
    try:
        stream = client.chat.completions.create(
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            messages=messages,
            stream=True,
            tools=tools,
            tool_choice="auto"
        )
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                response_chunks.append(content)
                yield content
            elif chunk.choices[0].delta.tool_calls:
                info = None
                # Tool í˜¸ì¶œ ì²˜ë¦¬
                tool_call = chunk.choices[0].delta.tool_calls[0]
                if tool_call.function.name == "get_product_list":
                    args = json.loads(tool_call.function.arguments)
                    info = get_product_list(args["product_name"], args["product_code"])
                

                if tool_call.function.name == "get_component_list":
                    args = json.loads(tool_call.function.arguments)
                    info = get_component_list(args["component_name"], args["component_code"] , args["component_maker"])

                if info is not None:
                    
                    messages = st.session_state.messages[:]
                    messages.insert(0 , {"role": "system", "content": system_prompt})

                    temp = messages.pop()
                    _temp = copy.deepcopy(temp)
                    _temp['content'] = f'###context:\n {info}\n\n###query: {temp["content"]}' 
                    messages.append(_temp)
                    
                    try:
                        stream = client.chat.completions.create(
                            model="gemma2-9b-it",
                            # model="mistral-saba-24b",
                            messages= messages, 
                            stream=True
                        )
                        for chunk in stream:
                            if chunk.choices[0].delta.content is not None:
                                content = chunk.choices[0].delta.content
                                response_chunks.append(content)
                                
                                yield content
                    except Exception as e:
                        error_msg = f"Error: {str(e)}"
                        print(error_msg)
                        response_chunks.append(error_msg)
                        yield error_msg
                    # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ ì „ì²´ ì‘ë‹µ ì €ì¥
                    st.session_state.messages.append({"role": "assistant", "content": "".join(response_chunks)})




    except Exception as e:
        error_msg = f"Error: {str(e)}"
        response_chunks.append(error_msg)
        yield error_msg

# left, right = st.columns((10, 20))

st.title("ğŸ’¬ Chat with AI")
st.info("ğŸš€ cherejeanne ì¬ê³  ê´€ë¦¬ ë“± ì—…ë¬´ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì„ AI ì—ê²Œ ë¬¸ì˜í•´ë³´ì„¸ìš”.")
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():

    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    with st.chat_message("assistant"):
        st.write_stream(get_openai_stream(prompt))